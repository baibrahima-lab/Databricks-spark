# üîç Projet Retail Expert Audit - PySpark

## üìã Description

Syst√®me d'analyse et d'audit de donn√©es retail utilisant PySpark pour le traitement distribu√© de grandes volum√©tries de donn√©es commerciales.

## üöÄ Installation Automatique

Le projet inclut un script d'installation automatique qui :

1. **Met √† jour les paquets syst√®me**
2. **Installe Java Runtime Environment (JRE)** - Version par d√©faut la plus stable
3. **Installe PySpark** - Framework de traitement distribu√©
4. **D√©tecte dynamiquement le chemin Java** - Compatible Google Colab et environnements vari√©s
5. **Configure les variables d'environnement** automatiquement

### Script d'Initialisation

```python
# ==============================================================================
# INSTALLATION ET R√âSOLUTION DYNAMIQUE DE JAVA
# ==============================================================================
# 1. Installation de la version Java par d√©faut (la plus s√ªre)
!apt-get update -qq
!apt-get install -y default-jre &gt; /dev/null

# 2. Installation de PySpark
!pip install -q pyspark

import os
import subprocess

# 3. Recherche automatique du chemin d'installation exact de Java sur Colab
try:
    # Demande au syst√®me Linux o√π pointe exactement la commande "java"
    java_exec_path = subprocess.check_output(["readlink", "-f", "/usr/bin/java"]).decode("utf-8").strip()

    # Remonte de deux dossiers (depuis /bin/java vers le dossier parent) pour avoir le JAVA_HOME
    java_home = os.path.dirname(os.path.dirname(java_exec_path))

    # Fixation de la variable d'environnement avec le chemin r√©el trouv√©
    os.environ["JAVA_HOME"] = java_home
    print(f"üîç Chemin Java trouv√© dynamiquement : {java_home}")

except Exception as e:
    print("‚ùå Impossible de trouver Java sur la machine :", e)

# 4. Lancement de PySpark
from pyspark.sql import SparkSession

try:
    spark = SparkSession.builder \
        .appName("Projet_Retail_Expert_Audit") \
        .master("local[*]") \
        .config("spark.sql.legacy.timeParserPolicy", "LEGACY") \
        .getOrCreate()



‚öôÔ∏è Configuration Spark

| Param√®tre                           | Valeur                       | Description                                |
| ----------------------------------- | ---------------------------- | ------------------------------------------ |
| `appName`                           | `Projet_Retail_Expert_Audit` | Nom de l'application                       |
| `master`                            | `local[*]`                   | Mode local avec tous les c≈ìurs disponibles |
| `spark.sql.legacy.timeParserPolicy` | `LEGACY`                     | Compatibilit√© avec anciens formats de date |



| Probl√®me             | Solution                                       |
| -------------------- | ---------------------------------------------- |
| `Java not found`     | V√©rifier que `default-jre` est bien install√©   |
| `Permission denied`  | Ex√©cuter avec `sudo` si n√©cessaire             |
| `SparkSession error` | V√©rifier la variable `JAVA_HOME` dans les logs |


üìù Notes

Le script utilise readlink -f pour r√©soudre les liens symboliques Java
La variable JAVA_HOME est d√©finie dynamiquement selon l'environnement
Optimis√© pour Google Colab mais fonctionne sur tout environnement Linux

üë®‚Äçüíª Auteur

BA Ibrahima, Mahamat
    print("‚úÖ La SparkSession est initialis√©e avec succ√®s.")
except Exception as e:
    print("‚ùå ERREUR au lancement de Spark :", e)
